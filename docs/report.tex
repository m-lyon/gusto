\documentclass[twocolumn, 9pt]{extarticle}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{lineno,hyperref}
\usepackage[table,x11names,dvipsnames,table]{xcolor}
\usepackage{authblk}
\usepackage{natbib}
\usepackage{subcaption,booktabs}
\usepackage{graphicx}
\usepackage{multirow}
\usepackage[nolist,nohyperlinks]{acronym}
\usepackage{tabularx}
\usepackage{cleveref}
\usepackage{float}
\usepackage[group-separator={,}]{siunitx}
\usepackage{geometry}
 \geometry{
 a4paper,
 papersize={210mm,279mm},
 left=12.73mm,
 top=20.3mm,
 marginpar=3.53mm,
 textheight=238.4mm,
 right=12.73mm,
}

\setlength{\columnsep}{6.54mm}

\renewcommand{\familydefault}{\sfdefault}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}

\captionsetup[figure]{labelfont=bf,textfont=normalfont}
\captionsetup[subfigure]{labelfont=bf,textfont=normalfont}

\makeatletter
\def\@maketitle{
\raggedright
\newpage
  \noindent
  \vspace{0cm}
  \let \footnote \thanks
    {\hskip -0.4em \huge \textbf{{\@title}} \par}
    \vskip 1.5em
    {\large
      \lineskip .5em
      \begin{tabular}[t]{l}
      \raggedright
        \@author
      \end{tabular}\par}
    \vskip 1em
  \par
  \vskip 1.5em
  }
\makeatother

\bibliographystyle{abbrvnat}
\setcitestyle{authoryear,open={(},close={)}}

\begin{document}

\title{GUSTO Analysis}

\author{Matthew Lyon}

\date{}
\maketitle

\section{Introduction}

This is a report on the analysis of the GUSTO data.

\section{Initial Model Development} \label{sec:initial_model}

These experiments use only a small subset of CpG sites as a pilot study. Specifically, only 383 CpG sites are used within these experiments. Each subject has 4 time points: 3 months, 9 months, 42 months, and 72 months. In this setting, the 3, 9, and 42 months are used to predict the 72nd month. The data are split into training, validation, and test sets with a 80/10/10 split.

\subsection{Time-series Autoencoder}

This set of experiments uses an autoencoder structure, which consists of a CNN encoder, an LSTM layer, and a CNN decoder. The encoder and decoder use 1D convolutions across the feature (CpG site) space, and treat each timepoint independently. As information is passed through the encoder, the feature dimension is reduced. The LSTM layer then processes the reduced feature space across time. Finally, the decoder expands the feature space back to the original dimension. A learned time embedding is added to the input and output of the LSTM to encode the time (in months) into the data. The model had 240K trainable parameters, and the exact number of layers and hyperparameters are detailed in the code, specifically within \texttt{AutoEncoder} class within \texttt{gusto/experiments/cnn\_autoencoder/lib/autoencoder.py}.

\subsubsection{Experiments}
Below are the experiments that were run. The code for each experiment is located in the \texttt{gusto/experiments/cnn\_autoencoder} directory. The models are trained using an $\ell_{1}$ loss function, and results are reported in terms of the root mean squared error (RMSE) on the validation set. the optimiser AdamW is used with a learning rate of $0.001$. The training and validation losses for experiments 1, 2, and 3 are shown in \Cref{fig:losses_cnn}.

\paragraph{Experiment 1} The first experiment trains and validates the aforementioned model. The code for this experiment is located in \texttt{experiment\_1.py}. The experiments that follow are modifications of this experiment.

\paragraph{Experiment 2} This experiment uses the same model as Experiment 1, but applies a weighting to the input timepoints as a function of the distance (in time) from the target timepoint. The code for this experiment is located in \texttt{experiment\_2.py}. This modification did not improve the performance of the model.

\paragraph{Experiment 3} This experiment uses the same model as Experiment 1, but interpolates the time dimension of the data. This is done by using a continuously differentiable sub-spline built from piecewise cubic polynomials, known as Akima splines, and implemented in \href{https://docs.scipy.org/doc/scipy/reference/generated/scipy.interpolate.Akima1DInterpolator.html\#scipy.interpolate.Akima1DInterpolator}{scipy}. The data are then resampled using a perturbation in the time dimension of the original 3, 9, and 42 month timepoints. The perturbation is drawn from a Gaussian distribution with a mean of 0 and a standard deviation of 3 months. The code for this experiment is located in \texttt{experiment\_3.py}. The rationale behind this was to allow the model to learn a more continuous representation of the time dimension. This modification did not improve the performance of the model.

\begin{figure}
  \centering
  \includegraphics[width=\columnwidth]{cnn_autoencoder_experiments_1_to_3.png}
  \caption{Training and validation losses for experiments 1, 2, and 3. Dotted lines indicate the training loss, and solid lines indicate the validation loss.}
  \label{fig:losses_cnn}
\end{figure}

\paragraph{Experiment 4} This experiment consisted of a hyperparameter search over the number of layers, kernel sizes, latent dimension size, and use of batch normalisation, within the previous three experiments. The code for this experiment is located in \texttt{experiment\_4.py}. The best hyperparameters, as determined by the validation loss, for each of the three experiments were then used to train a model on the full dataset. The training and validation losses for each of the three experiments are shown in \Cref{fig:losses_cnn_hparams}. Overall, the hyperparameter search for each experiment did not yield a significant improvement in the model performance.

\begin{figure}
  \centering
  \includegraphics[width=\columnwidth]{cnn_autoencoder_experiments_4.png}
  \caption{Training and validation losses for experiments 1, 2, and 3 with hyperparameters determined via hyperparameter search. Dotted lines indicate the training loss, and solid lines indicate the validation loss.}
  \label{fig:losses_cnn_hparams}
\end{figure}

% \subsection{Independent CpG Sites}

% This set of experiments used a small multi-layer perceptron (MLP) to predict the 72 month timepoint from the 3, 9, and 42 month timepoints for independent CpG sites. The network had three hidden layers, each with 32 units, and used a ReLU activation function. The code for this experiment is located in \texttt{gusto/experiments/independent\_cpg}.

% Experiment 1 trains this model on the full training dataset as specified within \Cref{sec:initial_model}, whilst Experiment 2 trains the model on CpG sites that have no missing data. The rationale behind this was to determine if the model could learn a better representation of the data without missing values. The results of these experiments are shown in \Cref{fig:independent_cpg}.

\subsection{Dictionary Learning}

This set of experiments uses a dictionary learning approach as the encoder and decoder within the autoencoder, whilst using an LSTM to perform timeseries prediction in the latent space. Here, the data is arranged as $\mathbf{X} \in \mathbb{R}^{M \times N}$, where $N$ is the number of samples and $M$ is the number of features. In this case, the number of features is $M = 383$, and $N$ is the number of subjects multiplied by the number of timepoints. Dictionary learning supposes the data can be represented as a linear combination of a set of basis vectors, known as atoms. The atoms form a dictionary $\mathbf{D} \in \mathbb{R}^{M \times K}$ which is learned from the training data, and $K$ is the size of the latent space. The coefficients $\boldsymbol{\alpha} \in \mathbb{R}^{K \times N}$ of the linear combination are learnt jointly using a gradient descent procedure, where the dictionary and coefficients are updated in a two-step process:

\begin{equation}
  \argmin_{\mathbf{D}, \boldsymbol{\alpha}} \left\| \mathbf{X} - \mathbf{D} \boldsymbol{\alpha} \right\|_{2}^{2} + \lambda \left\| \boldsymbol{\alpha} \right\|_{1},
\end{equation}

\noindent where $\lambda$ is a sparsity parameter, and $\left\| \boldsymbol{\alpha} \right\|_{1}$ acts as a sparsity constraint on the coefficients. Once the dictionary and coefficients are learned, an LSTM $f_{\theta}$ is used to predict the output timepoint coefficients from the input timepoint coefficients:

\begin{equation}
  \argmin_{\theta} \left\| f_{\theta}(\boldsymbol{\alpha}^{(\mathrm{input})}) - \boldsymbol{\alpha}^{(\mathrm{output})} \right\|_{2}^{2},
\end{equation}

Training this model involves a three-step process:

\begin{enumerate}
  \item Train the dictionary $\mathbf{D}$ and coefficients $\boldsymbol{\alpha}_{\mathrm{train}}$ on the training dataset $\mathbf{X}_{\mathrm{train}}$. Here, $\mathbf{X}_{\mathrm{train}} = \left[ \mathbf{X}_{\mathrm{train}}^{(\mathrm{input})}\quad \mathbf{X}_{\mathrm{train}}^{(\mathrm{output})} \right]$ contains both the input timepoints, and the output timepoints, for each subject.
  \item Freeze the dictionary $\mathbf{D}$ and train $\boldsymbol{\alpha}_{\mathrm{validation}}^{(\mathrm{input})}$ corresponding to only the input timepoints within the validation dataset $\mathbf{X}_{\mathrm{validation}}^{(\mathrm{input})}$.
  \item Freeze the dictionary $\mathbf{D}$, $\boldsymbol{\alpha}_{\mathrm{train}}$, and $\boldsymbol{\alpha}_{\mathrm{validation}}^{(\mathrm{input})}$. Learn $f_{\theta}$ by predicting $\boldsymbol{\alpha}_{\mathrm{train}}^{(\mathrm{output})}$ from $\boldsymbol{\alpha}_{\mathrm{train}}^{(\mathrm{input})}$. Validate the model by predicting $\boldsymbol{\alpha}_{\mathrm{validation}}^{(\mathrm{output})}$ from $\boldsymbol{\alpha}_{\mathrm{validation}}^{(\mathrm{input})}$.
\end{enumerate}

\subsubsection{Experiments}

Below are the experiments that were run using the dictionary learning approach. The code for each experiment is located in the \texttt{gusto/experiments/dict\_learning} directory. The training loss and validation loss for each of the models is shown in \Cref{fig:losses_dict}.

\begin{figure}
  \centering
  \includegraphics[width=\columnwidth]{dict_learning_experiments_1_to_3.png}
  \caption{Training and validation losses for experiments 1, 2, and 3. Dotted lines indicate the training loss, and solid lines indicate the validation loss.}
  \label{fig:losses_dict}
\end{figure}

\paragraph{Experiment 1} This experiment trains the dictionary learning model with a latent space size of 32, and $\lambda = 0.1$. The LSTM has 8.4K trainable parameters and the code for this experiment is located in \texttt{experiment\_1.py}.

\paragraph{Experiment 2} This experiment uses the same model as in Experiment 1, but with a dataset only consisting of examples with no missing data. Whilst the model performance was marginally better for most of the training, the model began to overfit around the 500 epoch mark. The code for this experiment is located in \texttt{experiment\_2.py}.

\paragraph{Experiment 3} This experiment uses a higher parameter LSTM model, and adds a periodic time encoding to the input and output of the LSTM. Specifically, this time encoding is a sinusoidal projection of the time dimension, as described as the positional encoding in \cite{vaswani2017attention}.

\bibliography{references}
\end{document}